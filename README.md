# SmartASL
In spite of being the 4th most commonly used language in the United States, sign language is actively used by only 10-14% of the members of the speech and hearing impairment community and continues to be one of the most understudied areas. In compliance with recent advances in deep learning, this paper explores the possibility of using deep convolutional neural network to interpret American Sign Language in real-time. The paper aims to develop a CNN from scratch and train it using a dataset patterned to closely match the format of the classic MNIST dataset (28x28 pixel images with pixel values ranging from 0-255). The evaluation of the network shows that it outperforms all previous implementations surrounding this task with 98.7% accuracy on validation set. To make the solution as accessible as possible, we refrained from using sophisticated hardware like motion tracking gloves and depth-sensing cameras and deployed the trained model as a multiplatform mobile application.

This prototype was developed as a 12hour hackathon build in Solve4Bharat hackathon organized by the PanIIT association in IISc Bangalore.
